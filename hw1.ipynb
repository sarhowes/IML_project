{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Timo van Essen HW1 INTML**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Week 2 Homework questions**\n",
    "\n",
    "1.1) Linear regression, house size, nr of bathrooms, construction year can all be abstracted to some polynomial function so we can use linear regression.\n",
    "\n",
    "1.2) Neither. We need some kind of information of similarity but is not given\n",
    "\n",
    "1.3) Linear regression, can again be abstracted to some polynomial equation\n",
    "\n",
    "1.4) Logistical regression, floods either happen or they dont this is a boolean problem\n",
    "\n",
    "1.5) Neither, but we could use linear regression to train an existing AI model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3. -6.  7. 24.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "# Load in table of data\n",
    "T = np.array([6, 20, 10, 21, 28])\n",
    "x = np.array([1, 2, 9, 4, -7])\n",
    "y = np.array([7, 4, 1, 6, 11])\n",
    "z = np.array([3, 2, -5, 3, 13])\n",
    "\n",
    "# Define the given function\n",
    "def func(X, theta_1, theta_2, theta_3, theta_4):\n",
    "    x, y, z = X\n",
    "    return theta_1*x + theta_2*y + theta_3*z + theta_4\n",
    "\n",
    "# Now we do linear regression to find the best-fit parameters\n",
    "theta_opt, _ = curve_fit(func, (x, y, z), T)\n",
    "print(theta_opt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1) Python code above\n",
    "\n",
    "2.2) If we have fewer than 4 datapoints then YES, we can still use least squares to approximate a solution. Its always better to have more datapoints, especially since this is 4 coupled linear equations we can find the exact solution using 4 datapoints. But even with 2 datapoints we can approximate the solution. (as you can test easily in python by deleting 3 datapoints for example)\n",
    "\n",
    "2.3) To be explicit final results theta1, 2, 3, 4 = (3, -6, 7, 24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1) In one vs one we train a classifier for all pairs of classes so we find n*(n-1)/2 classifiers (n-1 since we dont want to overcount by comparing against itself which is nonsense)\n",
    "In one vs all we just have n classifiers since we test each class once against all others.\n",
    "\n",
    "3.2) This absolutely does not mean one is necessarily faster than the other. In general it seems like one vs all should be 'better', but we can find exceptions easily. For example: if the seperate classes are small but there are very many classes then it could be impractical or even impossible to do one vs all, as it might not even be possible to load such a huge dataset into memory at once. Instead it would be faster to do lots of small comparisons by doing one vs one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Week 3 homework questions**\n",
    "\n",
    "1.1) if we do the entropy sum with p_g = p_b = 0.5 then we get IE(D) = 1\n",
    "\n",
    "1.2) For train we find 0.81,\n",
    "\n",
    "car 0 \n",
    "\n",
    "plane 1\n",
    "\n",
    "\n",
    "\n",
    "1.3) using the same formula again we find information gain IG(D, T) = 0.34\n",
    "\n",
    "1.4) IG(D, G) = 0.66\n",
    "\n",
    "IG(D, I) = 0.19\n",
    "\n",
    "1.5) Highest information gain is group size so we split on that.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1) We can transform a many-child DT into a binary DT. Consider a categorical variable with n categories, create n binary variables representing each category. Now build the binary tree by splitting for maximum information gain at each node, this ensures the tree will consider all the original categories. So we have a mapping of many-child to binary decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1) Because using the kernel function you can already compute the inner product between two given points in a lesser dimensional space (which is specified by the chosen kernel), which is the main reason SVMs work so well to begin with\n",
    "\n",
    "3.2) Just like an inner product the kernel function needs to be positive semi-definite as otherwise the SVM cannot do its optimization. If it allowed the inner product to have negative values (negative lengths, meaning not positive semi-definite) then we could not optimize for least length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Week 4 Homework Questions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1) I would use log loss to do the papaya tastiness problem, since it is a pretty safe bet to use on any binary classification problem.\n",
    "\n",
    "If we want to identify tasty from poisonous then it would be nice if we were REALLY sure that the papaya is not poisonous. So we can use something like focal loss function to make sure that we don't misclassify any poisonous papayas as tasty. Ofcourse this comes at the cost of classifiying some extra tasty papayas as poisonous. But better safe than sorry when working with life or death.\n",
    "\n",
    "If we turned it into a regression problem we could say measure the tastiness from 0 to 1 (arbitrary choice, 0 poisonous and 1 tasty) then I would probably use RMSE as loss function. It will treat large errors as a big penalty so its less likely to classify very poisonous papayas as tasty or vice-versa. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2.1) Here there is a clear problem of overfitting, since we have very many labels for binary classification. We also want to ensure we do regularization, because otherwise we can find significant feature correlations by simple chance (because theres so many features)\n",
    "\n",
    "1.2.2) This problem is highly likely seasonal so just a single month of data is not enough to predict the next and likely very different month. You can use the data from the old system or try to do your best by finding public data on seasonality in the hotel industry.\n",
    "\n",
    "1.2.3) So many features, overfitting is bound to happen. Also it would be good to only work with trees in the dataset that are similar to the one you want to classify. Maybe use PCA to remove lots of dimensionality and apply regularization. Or simply select a few features that work best.\n",
    "\n",
    "1.2.4) Some features on the bikes might not have a linear relationship to the price, like a few more gears could be a huge price increase.This is hard for a perceptron to classify correctly. We would want to do feature engineering or just use a different classifier here.\n",
    "\n",
    "1.2.5) The features are (obviously) not very well correlated with CO2 emissions, so we want to find features like engine volume, electric yes/no, tire width or vehicle weight instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
